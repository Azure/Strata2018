---
title: "Classifying Wiki Detox data"
output: html_notebook
---

```{r setup}
library(tidyverse)
library(tm)
library(pROC)

```

These are my notes of using bag of words and wordvector classifiers on the 'wiki detox' dataset. This script also featurizes the "wiki detox" datasets.

```{r load_data}
DATA_DIR <- "C:/Users/rhorton/Documents/conferences/StrataSJC2018/wiki_detox"

DATA_TARGET <- "attack"  # "attack", "aggression", "toxicity"

data_file <- paste0(DATA_TARGET, "_data.Rds")
flagged_column <- paste0("is_", DATA_TARGET)  # "is_attack"

dataset <- readRDS(file.path(DATA_DIR, data_file))[c("rev_id", "comment", flagged_column, "split")]
names(dataset)[2:3] <- c("text", "flagged")
```


## Fit Model

```{r fit_model}
in_training <- dataset$split == "test"

training_set <- dataset[in_training,]
test_set <- dataset[!in_training,]

L1 <- L2 <- 1
N_GRAMS <- 2
fit <- rxLogisticRegression(flagged ~ text_features, 
                       data=training_set, 
                       mlTransforms=list(
                        featurizeText(vars=c(text_features = "text"), 
                                      wordFeatureExtractor=ngramCount(ngramLength=N_GRAMS),
                                      keepPunctuations = FALSE,
                                      keepNumbers = FALSE)
                      ), 
                      l1Weight=L1, l2Weight=L2, 
                      type="binary",
                      verbose=0, reportProgress=0)

pred <- rxPredict(fit, test_set, extraVarsToWrite="flagged")

head(pred)

# library(pROC)
# roc_lm <- with(pred, roc(flagged, Probability))
# plot(roc_lm, print.auc=TRUE)

roc_lm <- rxRoc("flagged", "Probability", data=pred)
plot(roc_lm)

```
### Coefficients from linear model

```{r linear_model_coefficients}

coef(fit) %>% head(n=40)
```

```{r plot_roc_curves}
# confusion matrix
with(pred, table(flagged, PredictedLabel))

# accuracy
with(pred, sum(flagged == PredictedLabel)/length(flagged))

# AUC
rxAuc(roc_lm)

```


## Word embedding featurization

Keep only those that show up in our list of terms from the corpus.


```{r word_embedding_featurization}
library(tm)

WORDVEC_DIMENSIONS <- 50  # 50, 100, 200, 300
FULL_EMBEDDING_FILE <- sprintf("e:/embeddings/glove.6B/glove.6B.%dd.txt", WORDVEC_DIMENSIONS)
# "e:/embeddings/glove.6B/glove.6B.50d.txt"

WIKI_VOCAB_EMBEDDING_FILE <- sprintf("wiki_wordvecs_%dd.Rds", WORDVEC_DIMENSIONS)

TERM_FREQ_CONTROL <- list(removePunctuation=TRUE, 
                          removeNumbers=TRUE, 
                          stopWords=FALSE, 
                          tolower=TRUE, 
                          wordLengths=c(1, Inf))

if (! file.exists(WIKI_VOCAB_EMBEDDING_FILE)){
  corpus_term_frequencies <- termFreq(training_set$text, 
                                    control=TERM_FREQ_CONTROL) %>% sort(decreasing=TRUE)
  wordvecs <- read.delim(FULL_EMBEDDING_FILE, sep=" ", header=FALSE, row.names=1, quote="")

  dim(wordvecs) # 400000     50
  
  words_in_common <- intersect(row.names(wordvecs), names(corpus_term_frequencies))
  
  length(words_in_common) # 11551
  
  wordvecs <- wordvecs[words_in_common,]
  dim(wordvecs) # 11551    50
  
  saveRDS(wordvecs, WIKI_VOCAB_EMBEDDING_FILE)
} else {
  wordvecs <- readRDS(WIKI_VOCAB_EMBEDDING_FILE)
}


```

## Featurize text with word embeddings

```{r featurize_text}
FEATURIZED_DATA_FILE <- sprintf("featurized_wiki_comments_%s.Rds", DATA_TARGET)

if (file.exists(FEATURIZED_DATA_FILE)){
  predictors_df <- readRDS(FEATURIZED_DATA_FILE)
} else {
  get_combined_vector <- function(char_string, M){
    tryCatch({
      term_freq <- termFreq(char_string, control=TERM_FREQ_CONTROL)
      keepers <- intersect(names(term_freq), row.names(M))
      weights <- term_freq[keepers]
      vectors <- M[keepers,,drop=FALSE]
      for (v in seq_len(nrow(vectors))) vectors[v,] <- vectors[v,] * weights[v]
      colSums(vectors)/sum(weights)
    }, error = function(e) rep(0, ncol(M)))
  }
  
  X <- as.matrix(wordvecs)
  
  # predictors_df <- as.data.frame(t(
  #     vapply(dataset$text, get_combined_vector, numeric(ncol(wordvecs)), X)))
  
  # this is very slow either way ...
  predictors_df <- dataset$text %>%
      vapply(get_combined_vector, numeric(ncol(wordvecs)), X) %>%
      t %>%
      as.data.frame
  row.names(predictors_df) <- NULL
  
  #  Error in .tolower(txt) : invalid input 'elseðŸ˜Šthat' in 'utf8towcs' 
  
  predictors_df$rev_id <- dataset$rev_id
  predictors_df$flagged <- dataset$flagged
  
  saveRDS(predictors_df, FEATURIZED_DATA_FILE)
}

```

```{r train_linear_wordvec_model}
# Some lines contain no known words, leading to empty composite wordvectors.
dim(predictors_df)
incomplete_rows <- !complete.cases(predictors_df)
zero_rows <- (rowSums(predictors_df[1:50]) == 0)  & !incomplete_rows  # error in `get_combined_vector`
sum(incomplete_rows)
sum(zero_rows & !incomplete_rows)

training_vectors <- predictors_df[in_training & !incomplete_rows & !zero_rows,]
testing_vectors <- predictors_df[!in_training & !incomplete_rows & !zero_rows,]

form <- formula(paste("flagged", paste0('V', 2:51, collapse='+'), sep=' ~ '))
L1 <- 1e-8; L2 <- 1e-8
fit_vec <- rxLogisticRegression(form, training_vectors, type="binary", l1Weight=L1, l2Weight=L2)
pred_vec <- rxPredict(fit_vec, testing_vectors, extraVarsToWrite="flagged")

# confusion matrix
with(pred_vec, table(flagged, PredictedLabel))

# accuracy
with(pred_vec, sum(flagged==PredictedLabel, na.rm=TRUE)/nrow(testing_vectors))

# ROC
roc_lm_vec <- rxRoc("flagged", "Probability", data=pred_vec)
rxAuc(roc_lm_vec) # 0.8816423

plot(roc_lm_vec)

```

### Prediction from vectors with Neural Net

```{r predict_with_neural_net}
fit_nn_vec <- rxNeuralNet(form, training_vectors, type="binary", 
                      optimizer=adaDeltaSgd(), numIterations=150,
                      verbose=0, reportProgress=0)

pred_nn_vec <- rxPredict(fit_nn_vec, testing_vectors, extraVarsToWrite="flagged")

# confusion matrix
with(pred_nn_vec, table(flagged, PredictedLabel))

# accuracy
with(pred_nn_vec, sum(flagged==PredictedLabel, na.rm=TRUE)/sum(complete.cases(testing_vectors)))
# 100: 0.8969737
# 125: 0.8975807
# 150: 0.8994017
# 175: 0.8962258
# 200: 0.8557632

roc_nn_vec <- rxRoc("flagged", "Probability", data=pred_nn_vec)
rxAuc(roc_nn_vec)

plot(roc_nn_vec)

```

## Random Forest 
(does slightly better than neural net)
```{r random_forest}
library(randomForest)

rf_tvec <- training_vectors
rf_tvec$flagged <- factor(rf_tvec$flagged)
fit_rf <- randomForest(form, rf_tvec, ntree=501)

predictions <- predict(fit_rf, newdata=testing_vectors, type="prob")[,"TRUE"]

pred_df <- tibble(
  Probability = predictions,
  flagged = testing_vectors$flagged,
  predicted = predictions > 0.5
)
# pred_rf$flagged <- testing_vectors$flagged
# pred_rf$predicted <- names(pred_rf)[apply(pred_rf[1:3], 1, which.max)]

# accuracy
with(pred_df, sum(predicted == flagged)/nrow(pred_df)) # 0.9016237

# AUC
roc_rf_vec <- rxRoc("flagged", "Probability", data=pred_df)
rxAuc(roc_rf_vec) # 0.8496566 for regression, 0.8716459 for binary classification

plot(roc_rf_vec)

```


