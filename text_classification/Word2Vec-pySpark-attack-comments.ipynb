{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word vectors using word2vec: implementation in Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and create input output directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we show how to use Spark MLlib Word2Vec for generating word-features \n",
    "#### The data being used is the attack comments text data\n",
    "\n",
    "This notebook will take take about 1-2 mins to finish on a Python 3 Spark kernel on a DSVM with Spark\n",
    "\n",
    "MLlib Word2Vec: https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-04 03:36:39--  https://activelearning.blob.core.windows.net/activelearningdemo/text_data.zip\n",
      "Resolving activelearning.blob.core.windows.net (activelearning.blob.core.windows.net)... 13.77.184.64\n",
      "Connecting to activelearning.blob.core.windows.net (activelearning.blob.core.windows.net)|13.77.184.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64260679 (61M) [application/x-zip-compressed]\n",
      "Saving to: ‘text_data.zip’\n",
      "\n",
      "100%[======================================>] 64,260,679  26.4MB/s   in 2.3s   \n",
      "\n",
      "2018-03-04 03:36:42 (26.4 MB/s) - ‘text_data.zip’ saved [64260679/64260679]\n",
      "\n",
      "Archive:  text_data.zip\n",
      "   creating: text_data/\n",
      "  inflating: text_data/aggression_data.csv  \n",
      "  inflating: text_data/attack_data.csv  \n",
      "  inflating: text_data/toxicity_data.csv  \n"
     ]
    }
   ],
   "source": [
    "!cd /home/remoteuser/notebooks/SparkML\n",
    "!rm -r Outputs\n",
    "!mkdir Outputs\n",
    "\n",
    "!cd /home/remoteuser/notebooks/SparkML/Data\n",
    "!rm -r text_data.zip text_data\n",
    "!wget https://activelearning.blob.core.windows.net/activelearningdemo/text_data.zip\n",
    "!unzip text_data.zip\n",
    "\n",
    "!cd /home/remoteuser/notebooks/SparkML/pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set directory path for input data \n",
    "#### Input data is downloaded locally to a DSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Location of training data on \n",
    "text_file = \"/home/remoteuser/notebooks/SparkML/Data/text_data/attack_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set spark context and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, DataFrame, SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, regexp_replace, trim, col, lower, lit, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion: Read in attack text data from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## READ IN DATA AND CREATE SPARK DATAFRAME FROM A CSV & AND MATERIALIZE IN MEMORY\n",
    "text_df = spark.read.csv(path=text_file, header=True, inferSchema=True, sep=\",\")\n",
    "text_df.cache();\n",
    "\n",
    "## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\n",
    "text_df.createOrReplaceTempView(\"text_table\")\n",
    "\n",
    "# COUNT NUMBER OF ROWS IN DATA FRAME\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rev_id=37675, comment=\"`-NEWLINE_TOKENThis is not ``creative``.  Those are the dictionary definitions of the terms ``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you don't understand that, fine, legitimate criticism, I'll write up ``three man cell`` and ``bounty hunter`` and then it will be easy to understand why ``ensured`` and ``insured`` are different - and why both differ from ``assured``.NEWLINE_TOKENNEWLINE_TOKENThe sentence you quote is absolutely neutral.  You just aren't familiar with the underlying theory of strike-back (e.g. submarines as employed in nuclear warfare) guiding the insurance, nor likely the three man cell structure that kept the IRA from being broken by the British.  If that's my fault, fine, I can fix that to explain.  But ther'es nothing ``personal`` or ``creative`` about it.NEWLINE_TOKENNEWLINE_TOKENI'm tired of arguing with you.  Re: the other article, ``multi-party`` turns up plenty, and there is more use of ``mutually`` than ``mutual``.  If I were to apply your standard I'd be moving ``Mutual Assured Destruction`` to ``talk`` for not appealing to a Reagan voter's biases about its effectiveness, and for dropping the ``ly``.NEWLINE_TOKENNEWLINE_TOKENThere is a double standard in your edits.  If it comes from some US history book, like ``peace movement`` or 'M.A.D.' as defined in 1950, you like it, even if the definition is totally useless in 2002 and only of historical interest.  NEWLINE_TOKENNEWLINE_TOKENIf it makes any even-obvious connection or implication from the language chosen in multiple profession-specific terms, you consider it somehow non-neutral...  Gandhi thinks ``eye for an eye`` describes riots, death penalty, and war all at once, but you don't.  What do you know that Gandhi doesn't?NEWLINE_TOKENNEWLINE_TOKENGuess what:  reality is not neutral.  Current use of terms is slightly more controversial.  Neutrality requires negotiation, and some willingness to learn.NEWLINE_TOKENNEWLINE_TOKENThis is your problem not mine.  You may dislike the writing, fine, that can be fixed.  But disregarding fundamental axioms of philosphy with names that recur in multiple phrases, or failing to make critical distinctions like 'insurance' versus 'assurance' versus 'ensurance' (which are made in one quote by an Air Force general in an in-context quote), is just a disservice to the reader.NEWLINE_TOKENNEWLINE_TOKENIf someone comes here to research a topic like MAD, they want some context, beyond history.NEWLINE_TOKENNEWLINE_TOKENIf this is a history book, fine, it's a history book.  But that wasn't what it was claimed to be...NEWLINE_TOKEN`\", year='2002', logged_in='False', ns='article', sample='random', split='train', count='10', avg_attack=0.0, is_attack=False)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELECT ONE ROW FROM THE SPARK DATAFRAME\n",
    "text_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|     ns|split|\n",
      "+-------+-----+\n",
      "|article|  dev|\n",
      "|article| test|\n",
      "|article|train|\n",
      "|   user|  dev|\n",
      "|   user| test|\n",
      "|   user|train|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SELECT A FEW COLUMNS BASED ON WHICH WE FILTER\n",
    "sqlStatement = \"\"\" SELECT distinct ns, split \n",
    "            FROM text_table \n",
    "            where split in ('train', 'test', 'dev') \n",
    "            order by ns, split \"\"\"\n",
    "spark.sql(sqlStatement).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and filter data set (only training articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31253"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SELCT ONLY REV_ID AND COMMENT FIELDS, AND FILTER FOR TRAINING DATA AND ARTILES ONLY\n",
    "sqlStatement = \"\"\" SELECT rev_id, comment \n",
    "            FROM text_table \n",
    "            where ns = 'article' and split = 'train' \"\"\"\n",
    "text_filtered_df = spark.sql(sqlStatement)\n",
    "\n",
    "## CACHE NEW DATAFRAME IN MEMORY AND CREATE TEMPORARY TABLE\n",
    "text_filtered_df.cache(); \n",
    "text_filtered_df.createOrReplaceTempView(\"text_filtered_table\")\n",
    "\n",
    "## COUNT NUMBER OF ROWS IN DATAFRAME AFTER FILTERING\n",
    "text_filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase COMMENT and remove some irrelevant words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rev_id=37675, comment_final='this is not creative  those are the dictionary definitions of the terms insurance and ensurance as properly applied to destruction  if you dont understand that fine legitimate criticism ill write up three man cell and bounty hunter and then it will be easy to understand why ensured and insured are different - and why both differ from assuredthe sentence you quote is absolutely neutral  you just arent familiar with the underlying theory of strike-back eg submarines as employed in nuclear warfare guiding the insurance nor likely the three man cell structure that kept the ira from being broken by the british  if thats my fault fine i can fix that to explain  but theres nothing personal or creative about itim tired of arguing with you  re the other article multi-party turns up plenty and there is more use of mutually than mutual  if i were to apply your standard id be moving mutual assured destruction to talk for not appealing to a reagan voters biases about its effectiveness and for dropping the lythere is a double standard in your edits  if it comes from some us history book like peace movement or mad as defined in 1950 you like it even if the definition is totally useless in 2002 and only of historical interest  if it makes any even-obvious connection or implication from the language chosen in multiple profession-specific terms you consider it somehow non-neutral  gandhi thinks eye for an eye describes riots death penalty and war all at once but you dont  what do you know that gandhi doesntguess what  reality is not neutral  current use of terms is slightly more controversial  neutrality requires negotiation and some willingness to learnthis is your problem not mine  you may dislike the writing fine that can be fixed  but disregarding fundamental axioms of philosphy with names that recur in multiple phrases or failing to make critical distinctions like insurance versus assurance versus ensurance which are made in one quote by an air force general in an in-context quote is just a disservice to the readerif someone comes here to research a topic like mad they want some context beyond historyif this is a history book fine its a history book  but that wasnt what it was claimed to be')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filtered_df2 = text_filtered_df.withColumn(\"comment1\", lower(col(\"comment\"))).\\\n",
    "    withColumn(\"comment2\", regexp_replace(\"comment1\", '-newline_token', \"\")).\\\n",
    "    withColumn(\"comment3\", regexp_replace(\"comment2\", 'newline_token', \"\")).\\\n",
    "    withColumn(\"comment_final\", regexp_replace(\"comment3\", '[^\\w-_ ]', \"\")).\\\n",
    "    select('rev_id', 'comment_final')\n",
    "    \n",
    "\n",
    "# SELECT ONE ROW OF DATAFRAME AFTER \n",
    "text_filtered_df2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize COMMENT and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|rev_id|       comment_final|               words|           filtWords|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "| 37675|this is not creat...|[this, is, not, c...|[creative, , dict...|\n",
      "| 44816| the term standar...|[, the, term, sta...|[, term, standard...|\n",
      "| 49851|true or false the...|[true, or, false,...|[true, false, sit...|\n",
      "| 93890|this page will ne...|[this, page, will...|[page, need, disa...|\n",
      "|103624|i removed the fol...|[i, removed, the,...|[removed, followi...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DEFINE TOKENIZER AND STOPWORD REMOVER\n",
    "tokenizer = Tokenizer(inputCol=\"comment_final\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtWords\")\n",
    "\n",
    "## TRANSFORM DATASET TO TOKENIZE AND REMOVE STOPWORDS\n",
    "text_filtered_df3 = tokenizer.transform(text_filtered_df2)\n",
    "text_filtered_df4 = remover.transform(text_filtered_df3)\n",
    "\n",
    "## MATERIALIZE DATAFRAME IN MEMORY\n",
    "text_filtered_df4.cache(); text_filtered_df4.count();\n",
    "text_filtered_df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE AND RUN WORD2VEC ON COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib Word2Vec parameters: https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.mllib.feature.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "window_size = 5\n",
    "vector_size = 50\n",
    "min_count = 5\n",
    "\n",
    "## DEFINE WORD2VEC TRANSFORMER\n",
    "word2Vec = Word2Vec(windowSize = window_size, vectorSize = vector_size, minCount = min_count, inputCol=\"filtWords\", outputCol=\"result\")\n",
    "\n",
    "## FIT TRANSFORMER TO GENERATE FEATURES\n",
    "model = word2Vec.fit(text_filtered_df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some words, and other words close to them from these feature neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='bad'),\n",
       " Row(word='wikipediaassume'),\n",
       " Row(word='luck'),\n",
       " Row(word='assume')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.findSynonyms(\"good\", 20).select(\"word\").head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine how the vector features look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='quotient', vector=DenseVector([-0.0053, 0.005, -0.0141, 0.0217, 0.0175, 0.0126, 0.0187, -0.0265, 0.004, -0.0223, 0.0091, 0.0126, 0.0131, -0.0088, -0.0258, -0.0024, 0.0051, 0.0102, 0.0235, -0.0028, -0.0242, 0.0047, -0.0054, 0.0166, 0.0009, 0.0268, 0.0179, -0.0047, 0.0031, 0.0125, 0.0127, -0.0038, 0.0117, -0.0043, -0.0238, 0.0106, -0.0289, -0.0064, -0.0123, -0.0053, -0.0018, 0.005, 0.0017, 0.0005, 0.0239, -0.0248, -0.0397, 0.022, 0.0088, 0.0056]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_features = model.getVectors().select(\"*\")\n",
    "word2vec_features.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Spark DF to Pandas DF for output into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quotient</td>\n",
       "      <td>[-0.00533114839345, 0.00501936348155, -0.01409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incident</td>\n",
       "      <td>[0.0544441044331, -0.00731786200777, 0.1287074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serious</td>\n",
       "      <td>[-0.0265728700906, 0.0296862889081, -0.0313590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wgbh</td>\n",
       "      <td>[0.0179173815995, 0.0335741974413, 0.007226068...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                             vector\n",
       "0  quotient  [-0.00533114839345, 0.00501936348155, -0.01409...\n",
       "1  incident  [0.0544441044331, -0.00731786200777, 0.1287074...\n",
       "2   serious  [-0.0265728700906, 0.0296862889081, -0.0313590...\n",
       "3      wgbh  [0.0179173815995, 0.0335741974413, 0.007226068..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_features_pdf = word2vec_features.toPandas()\n",
    "word2vec_features_pdf.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features in CSV file for subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_features_pdf.to_csv('/home/remoteuser/notebooks/SparkML/Outputs/Word2Vec-Features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comment-level vectors from word-level vectors (averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rev_id=37675, result=DenseVector([0.0082, -0.0071, 0.0157, -0.0909, 0.027, -0.0496, 0.0008, -0.0288, 0.0032, 0.0279, -0.0248, 0.0324, 0.0443, 0.0007, 0.0277, 0.0036, 0.0136, -0.0455, -0.0064, 0.0321, 0.0276, -0.017, -0.0632, -0.004, -0.0205, -0.0301, 0.0031, 0.0796, -0.0117, 0.0063, 0.0142, 0.0277, -0.0569, 0.0596, -0.0351, 0.0615, 0.0386, -0.0099, 0.0155, -0.0365, -0.0126, -0.0222, 0.0584, 0.0329, -0.0064, -0.0027, 0.0637, 0.0007, -0.0114, 0.0059])),\n",
       " Row(rev_id=44816, result=DenseVector([0.0274, -0.0178, 0.0082, -0.0921, 0.0176, -0.0476, 0.0055, -0.0293, -0.0218, 0.0229, -0.0133, 0.0264, 0.0513, 0.0085, 0.0349, -0.0014, 0.0155, -0.0698, -0.0015, 0.0316, 0.0418, -0.0274, -0.0768, 0.0224, 0.0006, -0.024, -0.0154, 0.0918, -0.0431, 0.008, -0.0073, 0.0305, -0.0344, 0.0837, -0.0224, 0.0552, 0.0374, -0.0126, 0.0273, -0.0351, 0.0128, -0.039, 0.0374, 0.0291, 0.0234, -0.0086, 0.0545, -0.0094, -0.0097, -0.0128]))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_vectors_df = model.transform(text_filtered_df4).select('rev_id','result')\n",
    "comment_vectors_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
